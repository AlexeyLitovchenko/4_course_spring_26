{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V5E1",
      "authorship_tag": "ABX9TyOSum0QsuXw6Xnm/Ji5v2Du",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AlexeyLitovchenko/4_course_spring_26/blob/main/AutoEncoder.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Autoencoders, GANs, and Diffusion Models"
      ],
      "metadata": {
        "id": "i_VXXudcmhcu"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Wayv_4Hqwf83"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iDPfIxO2l0uX"
      },
      "outputs": [],
      "source": [
        "from packaging.version import Version\n",
        "import torch\n",
        "\n",
        "assert Version(torch.__version__) >= Version(\"2.6.0\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if torch.cuda.is_available():\n",
        "    device = \"cuda\"\n",
        "elif torch.backends.mps.is_available():\n",
        "    device = \"mps\"\n",
        "else:\n",
        "    device = \"cpu\"\n",
        "device"
      ],
      "metadata": {
        "id": "u0CC5PFJnpO8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.rc('font', size=14)\n",
        "plt.rc('axes', labelsize=14, titlesize=14)\n",
        "plt.rc('legend', fontsize=14)\n",
        "plt.rc('xtick', labelsize=10)\n",
        "plt.rc('ytick', labelsize=10)"
      ],
      "metadata": {
        "id": "XcnnRcEynpWG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchmetrics\n",
        "import torchmetrics"
      ],
      "metadata": {
        "id": "VwJ9th1mnpao"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_tm(model, data_loader, metric):\n",
        "    model.eval()\n",
        "    metric.reset()\n",
        "    with torch.no_grad():\n",
        "        for X_batch, y_batch in data_loader:\n",
        "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
        "            y_pred = model(X_batch)\n",
        "            if isinstance(y_pred, tuple):\n",
        "                y_pred = y_pred.output\n",
        "            metric.update(y_pred, y_batch)\n",
        "    return metric.compute()"
      ],
      "metadata": {
        "id": "CddDSg8kqHGY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, optimizer, loss_fn, metric, train_loader, valid_loader, n_epochs, patience=2, factor=0.5, epoch_callback=None):\n",
        "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "        optimizer, mode=\"max\", patience=patience, factor=factor\n",
        "    )\n",
        "    history = {\"train_losses\": [], \"train_metrics\": [], \"valid_metrics\": []}\n",
        "    for epoch in range(n_epochs):\n",
        "        total_loss = 0.0\n",
        "        metric.reset()\n",
        "        model.train()\n",
        "        if epoch_callback is not None:\n",
        "            epoch_callback(model, epoch)\n",
        "        for index, (X_batch, y_batch) in enumerate(train_loader):\n",
        "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
        "            y_pred = model(X_batch)\n",
        "            loss = loss_fn(y_pred, y_batch)\n",
        "            total_loss += loss.item()\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "            if isinstance(y_pred, tuple):\n",
        "                y_pred = y_pred.output\n",
        "            metric.update(y_pred, y_batch)\n",
        "            train_metric = metric.compute().item()\n",
        "            print(f\"\\rBatch {index + 1}/{len(train_loader)}\", end=\"\")\n",
        "            print(f\", loss={total_loss/(index+1):.4f}\", end=\"\")\n",
        "            print(f\", {train_metric=:.3f}\", end=\"\")\n",
        "        history[\"train_losses\"].append(total_loss / len(train_loader))\n",
        "        history[\"train_metrics\"].append(train_metric)\n",
        "        val_metric = evaluate_tm(model, valid_loader, metric).item()\n",
        "        history[\"valid_metrics\"].append(val_metric)\n",
        "        scheduler.step(val_metric)\n",
        "        print(f\"\\rEpoch {epoch + 1}/{n_epochs},                      \"\n",
        "              f\"train loss: {history['train_losses'][-1]:.4f}, \"\n",
        "              f\"train metric: {history['train_metrics'][-1]:.3}, \"\n",
        "              f\"valid metric: {history['valid_metrics'][-1]:.3}\")\n",
        "    return history"
      ],
      "metadata": {
        "id": "sIPeOlTBnpfe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Performing PCA with an Undercomplete Linear Autoencoder"
      ],
      "metadata": {
        "id": "2BrohMWvwSyq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "torch.manual_seed(42)\n",
        "encoder = nn.Linear(3, 2)\n",
        "decoder = nn.Linear(2, 3)\n",
        "autoencoder = nn.Sequential(encoder, decoder).to(device)"
      ],
      "metadata": {
        "id": "qh4uZ970npkA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy.spatial.transform import Rotation\n",
        "\n",
        "def generate_data(m, seed=42):\n",
        "    X = np.zeros((m, 3))  # initialize 3D dataset\n",
        "    rng = np.random.default_rng(seed)\n",
        "    angles = (rng.random(m) ** 3 + 0.5) * 2 * np.pi  # uneven distribution\n",
        "    X[:, 0], X[:, 1] = np.cos(angles), np.sin(angles) * 0.5  # oval\n",
        "    X += 0.28 * rng.standard_normal((m, 3))  # add more noise\n",
        "    X = Rotation.from_rotvec([np.pi / 29, -np.pi / 20, np.pi / 4]).apply(X)\n",
        "    X += [0.2, 0, 0.2]  # shift a bit\n",
        "    return torch.from_numpy(X.astype(np.float32))"
      ],
      "metadata": {
        "id": "7HEpMOn7npoh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "X_train = generate_data(60, seed=42)\n",
        "train_set = TensorDataset(X_train, X_train)\n",
        "train_loader = DataLoader(train_set, batch_size=32, shuffle=True)"
      ],
      "metadata": {
        "id": "drZEH4ElnptN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_valid = generate_data(500, seed=43)\n",
        "valid_set = TensorDataset(X_valid, X_valid)\n",
        "valid_loader = DataLoader(valid_set, batch_size=32)"
      ],
      "metadata": {
        "id": "j5K6UmOWnpyD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torchmetrics\n",
        "\n",
        "torch.manual_seed(42)\n",
        "optimizer = torch.optim.NAdam(autoencoder.parameters(), lr=0.2)\n",
        "mse = nn.MSELoss()\n",
        "rmse = torchmetrics.MeanSquaredError(squared=False).to(device)\n",
        "history = train(autoencoder, optimizer, mse, rmse, train_loader, valid_loader,\n",
        "                n_epochs=20)"
      ],
      "metadata": {
        "id": "9eXpNEHdxSrZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "codings = encoder(X_train.to(device))"
      ],
      "metadata": {
        "id": "0isNKnSFxSvw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "import numpy as np\n",
        "from scipy.spatial.transform import Rotation\n",
        "\n",
        "m = 60\n",
        "X = np.zeros((m, 3))  # initialize 3D dataset\n",
        "rng = np.random.default_rng(seed=42)\n",
        "angles = (rng.random(m) ** 3 + 0.5) * 2 * np.pi  # uneven distribution\n",
        "X[:, 0], X[:, 1] = np.cos(angles), np.sin(angles) * 0.5  # oval\n",
        "X += 0.28 * rng.standard_normal((m, 3))  # add more noise\n",
        "X = Rotation.from_rotvec([np.pi / 29, -np.pi / 20, np.pi / 4]).apply(X)\n",
        "X += [0.2, 0, 0.2]\n",
        "\n",
        "pca = PCA(n_components=2)\n",
        "X2D = pca.fit_transform(X)  # dataset reduced to 2D\n",
        "X3D_inv = pca.inverse_transform(X2D)  # 3D position of the projected samples\n",
        "X_centered = X - X.mean(axis=0)\n",
        "U, s, Vt = np.linalg.svd(X_centered)\n",
        "\n",
        "axes = [-1.4, 1.4, -1.4, 1.4, -1.1, 1.1]\n",
        "x1, x2 = np.meshgrid(np.linspace(axes[0], axes[1], 10),\n",
        "                     np.linspace(axes[2], axes[3], 10))\n",
        "w1, w2 = np.linalg.solve(Vt[:2, :2], Vt[:2, 2])  # projection plane coefs\n",
        "z = w1 * (x1 - pca.mean_[0]) + w2 * (x2 - pca.mean_[1]) - pca.mean_[2]  # plane\n",
        "X3D_above = X[X[:, 2] >= X3D_inv[:, 2]]  # samples above plane\n",
        "X3D_below = X[X[:, 2] < X3D_inv[:, 2]]  # samples below plane\n",
        "\n",
        "fig = plt.figure(figsize=(9, 9))\n",
        "ax = fig.add_subplot(111, projection=\"3d\")\n",
        "\n",
        "# plot samples and projection lines below plane first\n",
        "ax.plot(X3D_below[:, 0], X3D_below[:, 1], X3D_below[:, 2], \"ro\", alpha=0.3)\n",
        "for i in range(m):\n",
        "    if X[i, 2] < X3D_inv[i, 2]:\n",
        "        ax.plot([X[i][0], X3D_inv[i][0]],\n",
        "                [X[i][1], X3D_inv[i][1]],\n",
        "                [X[i][2], X3D_inv[i][2]], \":\", color=\"#F88\")\n",
        "\n",
        "ax.plot_surface(x1, x2, z, alpha=0.1, color=\"b\")  # projection plane\n",
        "ax.plot(X3D_inv[:, 0], X3D_inv[:, 1], X3D_inv[:, 2], \"b+\")  # projected samples\n",
        "ax.plot(X3D_inv[:, 0], X3D_inv[:, 1], X3D_inv[:, 2], \"b.\")\n",
        "\n",
        "# now plot projection lines and samples above plane\n",
        "for i in range(m):\n",
        "    if X[i, 2] >= X3D_inv[i, 2]:\n",
        "        ax.plot([X[i][0], X3D_inv[i][0]],\n",
        "                [X[i][1], X3D_inv[i][1]],\n",
        "                [X[i][2], X3D_inv[i][2]], \"r--\")\n",
        "\n",
        "ax.plot(X3D_above[:, 0], X3D_above[:, 1], X3D_above[:, 2], \"ro\")\n",
        "\n",
        "def set_xyz_axes(ax, axes):\n",
        "    ax.xaxis.set_rotate_label(False)\n",
        "    ax.yaxis.set_rotate_label(False)\n",
        "    ax.zaxis.set_rotate_label(False)\n",
        "    ax.set_xlabel(\"$x_1$\", labelpad=8, rotation=0)\n",
        "    ax.set_ylabel(\"$x_2$\", labelpad=8, rotation=0)\n",
        "    ax.set_zlabel(\"$x_3$\", labelpad=8, rotation=0)\n",
        "    ax.set_xlim(axes[0:2])\n",
        "    ax.set_ylim(axes[2:4])\n",
        "    ax.set_zlim(axes[4:6])\n",
        "\n",
        "set_xyz_axes(ax, axes)\n",
        "ax.set_zticks([-1, -0.5, 0, 0.5, 1])\n",
        "\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "dL5lsefa1Ep3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig = plt.figure(figsize=(4,3))\n",
        "codings_np = codings.cpu().detach().numpy()\n",
        "plt.plot(codings_np[:,0], codings_np[:, 1], \"b.\")\n",
        "plt.xlabel(\"$z_1$\", fontsize=18)\n",
        "plt.ylabel(\"$z_2$\", fontsize=18, rotation=0)\n",
        "plt.grid(True)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "5lT9VGEPxS0L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Stacked Autoencoders\n",
        "\n",
        "\n",
        "![image.png](data:image/png;base64,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)"
      ],
      "metadata": {
        "id": "wr6i3d4o1hNl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)  # extra code  ensures reproducibility\n",
        "\n",
        "stacked_encoder = nn.Sequential(\n",
        "    nn.Flatten(),\n",
        "    nn.Linear(1 * 28 * 28, 128), nn.ReLU(),\n",
        "    nn.Linear(128, 32), nn.ReLU(),\n",
        ")\n",
        "stacked_decoder = nn.Sequential(\n",
        "    nn.Linear(32, 128), nn.ReLU(),\n",
        "    nn.Linear(128, 1 * 28 * 28), nn.Sigmoid(),\n",
        "    nn.Unflatten(dim=1, unflattened_size=(1, 28, 28))\n",
        ")\n",
        "stacked_ae = nn.Sequential(stacked_encoder, stacked_decoder).to(device)"
      ],
      "metadata": {
        "id": "CIG0iCmRxS4A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision\n",
        "import torchvision.transforms.v2 as T\n",
        "\n",
        "toTensor = T.Compose([T.ToImage(), T.ToDtype(torch.float32, scale=True)])\n",
        "\n",
        "train_and_valid_data = torchvision.datasets.FashionMNIST(\n",
        "    root=\"datasets\", train=True, download=True, transform=toTensor)\n",
        "test_data = torchvision.datasets.FashionMNIST(\n",
        "    root=\"datasets\", train=False, download=True, transform=toTensor)\n",
        "\n",
        "torch.manual_seed(42)\n",
        "train_data, valid_data = torch.utils.data.random_split(\n",
        "    train_and_valid_data, [55_000, 5_000])"
      ],
      "metadata": {
        "id": "GUdye1JlxS78"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset\n",
        "\n",
        "class AutoencoderDataset(Dataset):\n",
        "    def __init__(self, base_dataset):\n",
        "        self.base_dataset = base_dataset\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.base_dataset)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        x, _ = self.base_dataset[idx]\n",
        "        return x, x\n",
        "\n",
        "train_loader = DataLoader(AutoencoderDataset(train_data), batch_size=32,\n",
        "                          shuffle=True)\n",
        "valid_loader = DataLoader(AutoencoderDataset(valid_data), batch_size=32)\n",
        "test_loader = DataLoader(AutoencoderDataset(test_data), batch_size=32)"
      ],
      "metadata": {
        "id": "2ZSV9HyyxTAA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.NAdam(stacked_ae.parameters(), lr=0.01)\n",
        "mse = nn.MSELoss()\n",
        "rmse = torchmetrics.MeanSquaredError(squared=False).to(device)\n",
        "history = train(stacked_ae, optimizer, mse, rmse, train_loader, valid_loader,\n",
        "                n_epochs=10)"
      ],
      "metadata": {
        "id": "CRaTJwIFxTDm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_image(image):\n",
        "    plt.imshow(image.permute(1, 2, 0).cpu(), cmap=\"binary\")\n",
        "    plt.axis(\"off\")\n",
        "\n",
        "def plot_reconstructions(model, images, n_images=5):\n",
        "    images = images[:n_images]\n",
        "    with torch.no_grad():\n",
        "        y_pred = model(images.to(device))\n",
        "    if isinstance(y_pred, tuple):\n",
        "        y_pred = y_pred.output\n",
        "    fig = plt.figure(figsize=(len(images) * 1.5, 3))\n",
        "    for idx in range(len(images)):\n",
        "        plt.subplot(2, len(images), 1 + idx)\n",
        "        plot_image(images[idx])\n",
        "        plt.subplot(2, len(images), 1 + len(images) + idx)\n",
        "        plot_image(y_pred[idx])"
      ],
      "metadata": {
        "id": "G9z4VP8q4GIY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_valid = torch.stack([x for x, _ in valid_data])\n",
        "plot_reconstructions(stacked_ae, X_valid)\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "66ygppnS4GLr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Using AutoEncoders for Anomaly Detection"
      ],
      "metadata": {
        "id": "7gl-1Xl94J--"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)\n",
        "mnist_data = torchvision.datasets.MNIST(\n",
        "    root=\"datasets\", train=True, download=True, transform=toTensor)\n",
        "mnist_images = torch.stack([mnist_data[i][0] for i in range(X_valid.size(0))])\n",
        "plot_reconstructions(stacked_ae, images=mnist_images)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "DWtfnL5q4GOD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "images = mnist_images.to(device)\n",
        "with torch.no_grad():\n",
        "    y_pred = stacked_ae(images)\n",
        "    recon_loss = torch.nn.functional.mse_loss(y_pred, images)\n",
        "\n",
        "recon_loss"
      ],
      "metadata": {
        "id": "3e9IhBFc4GQq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "def compute_reconstruction_losses(X, device):\n",
        "    X = X.to(device)\n",
        "    with torch.no_grad():\n",
        "        y_pred = stacked_ae(X)\n",
        "        return F.mse_loss(y_pred, X, reduction=\"none\").view(X.size(0), -1).mean(dim=1).cpu()\n",
        "\n",
        "recon_losses_mnist = compute_reconstruction_losses(mnist_images, device)\n",
        "recon_losses_fashion_mnist = compute_reconstruction_losses(X_valid, device)\n",
        "\n",
        "plt.hist(recon_losses_mnist, bins=85, alpha=0.8, label=\"MNIST images\")\n",
        "plt.hist(recon_losses_fashion_mnist, bins=85, alpha=0.8, label=\"Fashion MNIST images\")\n",
        "plt.xlabel(\"Reconstruction loss (MSE)\")\n",
        "plt.ylabel(\"Count\", rotation=0)\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "OBB5G-n94GTB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Visualizing the Fashion MNIST Dataset"
      ],
      "metadata": {
        "id": "q-m0eX174VZB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.manifold import TSNE\n",
        "\n",
        "with torch.no_grad():\n",
        "    X_valid_compressed = stacked_encoder(X_valid.to(device))\n",
        "\n",
        "tsne = TSNE(init=\"pca\", learning_rate=\"auto\", random_state=42)\n",
        "X_valid_2D = tsne.fit_transform(X_valid_compressed.cpu())"
      ],
      "metadata": {
        "id": "Xf8zj6qN4Xqe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_valid = torch.tensor([y for _, y in valid_data])\n",
        "\n",
        "plt.scatter(X_valid_2D[:, 0], X_valid_2D[:, 1], c=y_valid, s=10, cmap=\"tab10\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "a2OfSJEL4Xtd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib as mpl\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "cmap = plt.cm.tab10\n",
        "Z = X_valid_2D\n",
        "Z = (Z - Z.min()) / (Z.max() - Z.min())  # normalize to the 0-1 range\n",
        "plt.scatter(Z[:, 0], Z[:, 1], c=y_valid, s=10, cmap=cmap)\n",
        "image_positions = np.array([[1., 1.]])\n",
        "for index, position in enumerate(Z):\n",
        "    dist = ((position - image_positions) ** 2).sum(axis=1)\n",
        "    if dist.min() > 0.02: # if far enough from other images\n",
        "        image_positions = np.r_[image_positions, [position]]\n",
        "        imagebox = mpl.offsetbox.AnnotationBbox(\n",
        "            mpl.offsetbox.OffsetImage(X_valid[index].squeeze(dim=0),\n",
        "                                      cmap=\"binary\"),\n",
        "            position, bboxprops={\"edgecolor\": cmap(y_valid[index]), \"lw\": 2})\n",
        "        plt.gca().add_artist(imagebox)\n",
        "\n",
        "plt.axis(\"off\")\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ZEcOCXWU4Xwj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Convolutional Autoencoders"
      ],
      "metadata": {
        "id": "DZ3EkFim4ykM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)  # extra code  ensures reproducibility\n",
        "\n",
        "conv_encoder = nn.Sequential(\n",
        "    nn.Conv2d(1, 16, kernel_size=3, padding=\"same\"), nn.ReLU(),\n",
        "    nn.MaxPool2d(kernel_size=2),  # output: 16  14  14\n",
        "    nn.Conv2d(16, 32, kernel_size=3, padding=\"same\"), nn.ReLU(),\n",
        "    nn.MaxPool2d(kernel_size=2),  # output: 32  7  7\n",
        "    nn.Conv2d(32, 64, kernel_size=3, padding=\"same\"), nn.ReLU(),\n",
        "    nn.MaxPool2d(kernel_size=2),  # output: 64  3  3\n",
        "    nn.Conv2d(64, 32, kernel_size=3, padding=\"same\"), nn.ReLU(),\n",
        "    nn.AdaptiveAvgPool2d((1, 1)), nn.Flatten())  # output: 32\n",
        "\n",
        "conv_decoder = nn.Sequential(\n",
        "    nn.Linear(32, 16 * 3 * 3),\n",
        "    nn.Unflatten(dim=1, unflattened_size=(16, 3, 3)),\n",
        "    nn.ConvTranspose2d(16, 32, kernel_size=3, stride=2), nn.ReLU(),\n",
        "    nn.ConvTranspose2d(32, 16, kernel_size=3, stride=2, padding=1,\n",
        "                       output_padding=1), nn.ReLU(),\n",
        "    nn.ConvTranspose2d(16, 1, kernel_size=3, stride=2, padding=1,\n",
        "                       output_padding=1), nn.Sigmoid())\n",
        "\n",
        "conv_ae = nn.Sequential(conv_encoder, conv_decoder).to(device)"
      ],
      "metadata": {
        "id": "FwMtDfcZ4Xzd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.NAdam(conv_ae.parameters(), lr=0.005)\n",
        "history = train(conv_ae, optimizer, mse, rmse, train_loader, valid_loader,\n",
        "                n_epochs=10)"
      ],
      "metadata": {
        "id": "XF9SJ5rU4X2S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# extra code  shows the reconstructions\n",
        "plot_reconstructions(conv_ae, X_valid)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "NZc701pz4X5K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Extra Material  Recurrent Autoencoders"
      ],
      "metadata": {
        "id": "rtVvqGIs47vP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class RecurrentAutoencoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.encoder_lstm = nn.LSTM(input_size=28, hidden_size=128,\n",
        "                                    num_layers=2, batch_first=True)\n",
        "        self.encoder_proj = nn.Linear(128, 32)  # Compress to latent vector\n",
        "        self.decoder_lstm = nn.LSTM(input_size=32, hidden_size=128,\n",
        "                                    batch_first=True)\n",
        "        self.decoder_proj = nn.Linear(128, 28)\n",
        "\n",
        "    def encode(self, X):  # X shape: [B, 1, 28, 28]\n",
        "        Z = X.squeeze(dim=1)  # Z shape: [B, 28, 28]\n",
        "        _, (h_n, _) = self.encoder_lstm(Z)  # h_n shape: [2, B, 100]\n",
        "        Z = h_n[-1]  # get the hidden state of the last layer: [B, 100]\n",
        "        return self.encoder_proj(Z)  # [B, 30]\n",
        "\n",
        "    def decode(self, X):\n",
        "        Z = X.unsqueeze(dim=1).repeat(1, 28, 1)  # [B, 28, 32]\n",
        "        Z, _ = self.decoder_lstm(Z)  # [B, 28, 100]\n",
        "        return F.sigmoid(self.decoder_proj(Z).unsqueeze(dim=1)) # [B, 1, 28, 28]\n",
        "\n",
        "    def forward(self, X):\n",
        "        return self.decode(self.encode(X))\n",
        "\n",
        "torch.manual_seed(42)\n",
        "recurrent_ae = RecurrentAutoencoder().to(device)"
      ],
      "metadata": {
        "id": "ls7CqFao4X7m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.NAdam(recurrent_ae.parameters(), lr=1e-3)\n",
        "history = train(recurrent_ae, optimizer, mse, rmse, train_loader, valid_loader,\n",
        "                n_epochs=10)"
      ],
      "metadata": {
        "id": "m7Uwgb3Z4GVa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Denoising Autoencoders\n",
        "\n",
        "Using dropout -> Instead of using Dropout, we can add Gaussian noise"
      ],
      "metadata": {
        "id": "oYtYvyV55B_F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)  # extra code  ensures reproducibility\n",
        "\n",
        "dropout_encoder = nn.Sequential(\n",
        "    nn.Flatten(),\n",
        "    nn.Dropout(0.5),\n",
        "    nn.Linear(1 * 28 * 28, 128), nn.ReLU(),\n",
        "    nn.Linear(128, 128), nn.ReLU(),\n",
        ")\n",
        "dropout_decoder = nn.Sequential(\n",
        "    nn.Linear(128, 128), nn.ReLU(),\n",
        "    nn.Linear(128, 1 * 28 * 28), nn.Sigmoid(),\n",
        "    nn.Unflatten(dim=1, unflattened_size=(1, 28, 28))\n",
        ")\n",
        "dropout_ae = nn.Sequential(dropout_encoder, dropout_decoder).to(device)"
      ],
      "metadata": {
        "id": "lFr2h0VP5Kk3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.NAdam(dropout_ae.parameters(), lr=0.01)\n",
        "history = train(dropout_ae, optimizer, mse, rmse, train_loader, valid_loader,\n",
        "                n_epochs=10)"
      ],
      "metadata": {
        "id": "NM2Isc8O5Kpo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# extra code  this cell generates Figure 189\n",
        "torch.manual_seed(42)\n",
        "dropout = nn.Dropout(0.5)\n",
        "plot_reconstructions(dropout_ae, dropout(X_valid))\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "jcZjlLqH5Ksk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GaussianNoise(nn.Module):\n",
        "    def __init__(self, std):\n",
        "        super().__init__()\n",
        "        self.std = std\n",
        "\n",
        "    def forward(self, X):\n",
        "        if self.training:  # only add noise during training\n",
        "            noise = torch.randn_like(X) * self.std\n",
        "            return X + noise\n",
        "        return X"
      ],
      "metadata": {
        "id": "TGb63rpr5Kvd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)  # extra code  ensures reproducibility\n",
        "\n",
        "noise_encoder = nn.Sequential(\n",
        "    nn.Flatten(),\n",
        "    GaussianNoise(0.5),\n",
        "    nn.Linear(1 * 28 * 28, 128), nn.ReLU(),\n",
        "    nn.Linear(128, 128), nn.ReLU(),\n",
        ")\n",
        "noise_decoder = nn.Sequential(\n",
        "    nn.Linear(128, 128), nn.ReLU(),\n",
        "    nn.Linear(128, 1 * 28 * 28), nn.Sigmoid(),\n",
        "    nn.Unflatten(dim=1, unflattened_size=(1, 28, 28))\n",
        ")\n",
        "noise_ae = nn.Sequential(noise_encoder, noise_decoder).to(device)"
      ],
      "metadata": {
        "id": "CXs2jUj45KyK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.NAdam(noise_ae.parameters(), lr=0.01)\n",
        "history = train(noise_ae, optimizer, mse, rmse, train_loader, valid_loader,\n",
        "                n_epochs=10)"
      ],
      "metadata": {
        "id": "WGhDynM_5K0v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# extra code\n",
        "torch.manual_seed(42)\n",
        "noise = GaussianNoise(0.5)\n",
        "plot_reconstructions(noise_ae, noise(X_valid))\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "FU9gNlCv5K2_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Sparse Autoencoder\n",
        "\n",
        "\n",
        "Let's use the sigmoid activation function in the coding layer. Let's also add\n",
        " regularization to it: to do this, we make the module return both the reconstructions and the encodings (i.e., the output of the encoder), so the regularization loss can be computed based on the encodings."
      ],
      "metadata": {
        "id": "Z6YFVvpr5UEy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import namedtuple\n",
        "\n",
        "AEOutput = namedtuple(\"AEOutput\", [\"output\", \"codings\"])\n",
        "\n",
        "class SparseAutoencoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(1 * 28 * 28, 128), nn.ReLU(),\n",
        "            nn.Linear(128, 256), nn.Sigmoid())\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Linear(256, 128), nn.ReLU(),\n",
        "            nn.Linear(128, 1 * 28 * 28), nn.Sigmoid(),\n",
        "            nn.Unflatten(dim=1, unflattened_size=(1, 28, 28)))\n",
        "\n",
        "    def forward(self, X):\n",
        "        codings = self.encoder(X)\n",
        "        output = self.decoder(codings)\n",
        "        return AEOutput(output, codings)"
      ],
      "metadata": {
        "id": "uLNTEEtt5XFi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def mse_plus_sparsity_loss(y_pred, y_target, target_sparsity=0.1,\n",
        "                           kl_weight=1e-3, eps=1e-8):\n",
        "    p = torch.tensor(target_sparsity, device=y_pred.codings.device)\n",
        "    q = torch.clamp(y_pred.codings.mean(dim=0), eps, 1 - eps)  # actual sparsity\n",
        "    kl_div = p * torch.log(p / q) + (1 - p) * torch.log((1 - p) / (1 - q))\n",
        "    return mse(y_pred.output, y_target) + kl_weight * kl_div.sum()"
      ],
      "metadata": {
        "id": "VRUs_Oyg5XIS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's plot the KL Divergence loss, versus the MAE and MSE:\n",
        "plt.figure(figsize=(6, 3.5))\n",
        "p = 0.1\n",
        "q = np.linspace(0.001, 0.999, 500)\n",
        "kl_div = p * np.log(p / q) + (1 - p) * np.log((1 - p) / (1 - q))\n",
        "mse_ = (p - q) ** 2\n",
        "mae = np.abs(p - q)\n",
        "plt.plot([p, p], [0, 0.3], \"k:\")\n",
        "plt.text(0.05, 0.32, \"Target\\nsparsity $p$\", fontsize=14)\n",
        "plt.plot(q, kl_div, \"b-\", label=\"KL divergence\")\n",
        "plt.plot(q, mae, \"g--\", label=r\"MAE ($\\ell_1$)\")\n",
        "plt.plot(q, mse_, \"r--\", linewidth=1, label=r\"MSE ($\\ell_2$)\")\n",
        "plt.legend(loc=\"upper left\", fontsize=14)\n",
        "plt.xlabel(\"Actual sparsity $q$\")\n",
        "plt.ylabel(\"Cost\", rotation=0)\n",
        "plt.axis([0, 1, 0, 0.95])\n",
        "plt.grid(True)"
      ],
      "metadata": {
        "id": "lWtB18aG5XLF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)\n",
        "sparse_ae = SparseAutoencoder().to(device)\n",
        "optimizer = torch.optim.NAdam(sparse_ae.parameters(), lr=0.002)\n",
        "history = train(sparse_ae, optimizer, mse_plus_sparsity_loss, rmse,\n",
        "                train_loader, valid_loader, n_epochs=10)"
      ],
      "metadata": {
        "id": "AsaP1F745XNz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# extra code  shows the reconstructions\n",
        "plot_reconstructions(sparse_ae, X_valid)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "gANAttPs5XQn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "    y_pred = sparse_ae(X_valid.to(device))\n",
        "\n",
        "encs = y_pred.codings.flatten().detach().cpu()\n",
        "plt.hist(encs, log=True)\n",
        "plt.xlabel(\"Activation\")\n",
        "plt.ylabel(\"Count\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "1XKl2iDM5XTQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred.codings.mean()"
      ],
      "metadata": {
        "id": "G1ID7Rsl5XWD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_multiple_images(images, n_cols=None):\n",
        "    n_cols = n_cols or len(images)\n",
        "    n_rows = (len(images) - 1) // n_cols + 1\n",
        "    plt.figure(figsize=(n_cols, n_rows))\n",
        "    for index, image in enumerate(images):\n",
        "        plt.subplot(n_rows, n_cols, index + 1)\n",
        "        plot_image(image)\n",
        "\n",
        "dim = 6\n",
        "codings = y_pred.codings[:, dim].cpu()\n",
        "threshold = np.percentile(codings, 90)\n",
        "selected_images = X_valid[codings > threshold]\n",
        "plot_multiple_images(selected_images[:35], 7)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "su2znfSx5XY5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Variational Autoencoder"
      ],
      "metadata": {
        "id": "Vq20jN3k6AKl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "VAEOutput = namedtuple(\"VAEOutput\",\n",
        "                       [\"output\", \"codings_mean\", \"codings_logvar\"])\n",
        "\n",
        "class VAE(nn.Module):\n",
        "    def __init__(self, codings_dim=32):\n",
        "        super(VAE, self).__init__()\n",
        "        self.codings_dim = codings_dim\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(1 * 28 * 28, 128), nn.ReLU(),\n",
        "            nn.Linear(128, 2 * codings_dim))  # output both the mean and logvar\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Linear(codings_dim, 128), nn.ReLU(),\n",
        "            nn.Linear(128, 1 * 28 * 28), nn.Sigmoid(),\n",
        "            nn.Unflatten(dim=1, unflattened_size=(1, 28, 28)))\n",
        "\n",
        "    def encode(self, X):\n",
        "        return self.encoder(X).chunk(2, dim=-1)  # returns (mean, logvar)\n",
        "\n",
        "    def sample_codings(self, codings_mean, codings_logvar):\n",
        "        codings_std = torch.exp(0.5 * codings_logvar)\n",
        "        noise = torch.randn_like(codings_std)\n",
        "        return codings_mean + noise * codings_std\n",
        "\n",
        "    def decode(self, Z):\n",
        "        return self.decoder(Z)\n",
        "\n",
        "    def forward(self, X):\n",
        "        codings_mean, codings_logvar = self.encode(X)\n",
        "        codings = self.sample_codings(codings_mean, codings_logvar)\n",
        "        output = self.decode(codings)\n",
        "        return VAEOutput(output, codings_mean, codings_logvar)"
      ],
      "metadata": {
        "id": "ohdAEYky5XbW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def vae_loss(y_pred, y_target, kl_weight=1.0):\n",
        "    output, mean, logvar = y_pred\n",
        "    kl_div = -0.5 * torch.sum(1 + logvar - logvar.exp() - mean.square(), dim=-1)\n",
        "    return F.mse_loss(output, y_target) + kl_weight * kl_div.mean() / 784"
      ],
      "metadata": {
        "id": "OtIeSVSL6EOJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)\n",
        "vae = VAE().to(device)\n",
        "optimizer = torch.optim.NAdam(vae.parameters(), lr=1e-3)\n",
        "history = train(vae, optimizer, vae_loss, rmse, train_loader, valid_loader,\n",
        "                n_epochs=20)"
      ],
      "metadata": {
        "id": "rD3Kr7LA6EQ7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_reconstructions(vae, X_valid)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "KY6b5TzY6ETt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generate Fashion Images"
      ],
      "metadata": {
        "id": "GDoXWoFv6IzK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)  # extra code  ensures reproducibility\n",
        "\n",
        "vae.eval()\n",
        "codings = torch.randn(3 * 7, vae.codings_dim, device=device)\n",
        "with torch.no_grad():\n",
        "    images = vae.decode(codings)"
      ],
      "metadata": {
        "id": "ZH3b-lLG6EWY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# extra code  this cells generates Figure 18-12\n",
        "\n",
        "plot_multiple_images(images, 7)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "nAkK5DAk6EY5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "codings.shape"
      ],
      "metadata": {
        "id": "t2UceQbW6EbH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(111)  # extra code  ensure reproducibility\n",
        "\n",
        "codings = torch.randn(2, vae.codings_dim)  # start and end codings\n",
        "n_images = 7\n",
        "weights = torch.linspace(0, 1, n_images).view(n_images, 1)\n",
        "codings = torch.lerp(codings[0], codings[1], weights)  # linear interpolation\n",
        "with torch.no_grad():\n",
        "    images = vae.decode(codings.to(device))"
      ],
      "metadata": {
        "id": "0M2kO7gK6OWN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_multiple_images(images)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Myk6apZJ6OZD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Discrete VAE"
      ],
      "metadata": {
        "id": "uYOCrOlk6QbT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def gumbel_softmax(logits, tau=1, hard=False, dim=-1):\n",
        "    if device != \"mps\":\n",
        "        return F.gumbel_softmax(logits, tau, hard, dim)\n",
        "    gumbels = (\n",
        "        -torch.empty_like(logits, memory_format=torch.legacy_contiguous_format)\n",
        "        .exponential_()\n",
        "        .log()\n",
        "    )  # ~Gumbel(0,1)\n",
        "    gumbels = torch.clamp(gumbels, -30, 30)  # <<<<<<<<<<<<<<<<<<<<< ADDED\n",
        "    gumbels = (logits + gumbels) / tau  # ~Gumbel(logits,tau)\n",
        "    y_soft = gumbels.softmax(dim)\n",
        "\n",
        "    if hard:\n",
        "        # Straight through.\n",
        "        index = y_soft.max(dim, keepdim=True)[1]\n",
        "        y_hard = torch.zeros_like(\n",
        "            logits, memory_format=torch.legacy_contiguous_format\n",
        "        ).scatter_(dim, index, 1.0)\n",
        "        ret = y_hard - y_soft.detach() + y_soft\n",
        "    else:\n",
        "        # Reparametrization trick.\n",
        "        ret = y_soft\n",
        "    return ret"
      ],
      "metadata": {
        "id": "IG2Lazcw6Oc0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DiscreteVAEOutput = namedtuple(\"DiscreteVAEOutput\",\n",
        "                               [\"output\", \"logits\", \"codings_prob\"])\n",
        "\n",
        "class DiscreteVAE(nn.Module):\n",
        "    def __init__(self, coding_length=32, n_codes=16, temperature=1.0):\n",
        "        super().__init__()\n",
        "        self.coding_length = coding_length\n",
        "        self.n_codes = n_codes\n",
        "        self.temperature = temperature\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(1 * 28 * 28, 256), nn.ReLU(),\n",
        "            nn.Linear(256, 128), nn.ReLU(),\n",
        "            nn.Linear(128, coding_length * n_codes),\n",
        "            nn.Unflatten(dim=1, unflattened_size=(coding_length, n_codes)))\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(coding_length * n_codes, 128), nn.ReLU(),\n",
        "            nn.Linear(128, 256), nn.ReLU(),\n",
        "            nn.Linear(256, 1 * 28 * 28), nn.Sigmoid(),\n",
        "            nn.Unflatten(dim=1, unflattened_size=(1, 28, 28)))\n",
        "\n",
        "    def forward(self, X):\n",
        "        logits = self.encoder(X)\n",
        "        codings_prob = gumbel_softmax(logits, tau=self.temperature, hard=True)\n",
        "        output = self.decoder(codings_prob)\n",
        "        return DiscreteVAEOutput(output, logits, codings_prob)"
      ],
      "metadata": {
        "id": "j43ReXV06Ofg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def d_vae_loss(y_pred, y_target, kl_weight=1.0):\n",
        "    output, logits, _ = y_pred\n",
        "    codings_prob = F.softmax(logits, -1)\n",
        "    k = logits.new_tensor(logits.size(-1))  # same device and dtype as logits\n",
        "    kl_div = (codings_prob * (codings_prob.log() + k.log())).sum(dim=(1, 2))\n",
        "    return F.mse_loss(output, y_target) + kl_weight * kl_div.mean() / 784"
      ],
      "metadata": {
        "id": "7RxE7sNg6OiX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_epochs = 20\n",
        "def annealing(model, epoch):\n",
        "    model.temperature = 1 - 0.9 * epoch / n_epochs\n",
        "\n",
        "torch.manual_seed(42)\n",
        "d_vae = DiscreteVAE().to(device)\n",
        "optimizer = torch.optim.NAdam(d_vae.parameters(), lr=0.001)\n",
        "history = train(d_vae, optimizer, d_vae_loss, rmse, train_loader, valid_loader,\n",
        "                n_epochs=n_epochs, epoch_callback=annealing)"
      ],
      "metadata": {
        "id": "aWstnUv_6Ok4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)  # extra code  ensures reproducibility\n",
        "n_images = 3 * 7\n",
        "codings = torch.randint(0, d_vae.n_codes,  # from 0 to k  1\n",
        "                        (n_images, d_vae.coding_length), device=device)\n",
        "codings_prob = F.one_hot(codings, num_classes=d_vae.n_codes).float()\n",
        "with torch.no_grad():\n",
        "    images = d_vae.decoder(codings_prob)"
      ],
      "metadata": {
        "id": "UmI065qN6OnJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_multiple_images(images, 7)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "takvSCTu6bkU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generative Adversarial Networks"
      ],
      "metadata": {
        "id": "2mKZwyEK6dAh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)  # extra code  ensures reproducibility\n",
        "\n",
        "codings_dim = 32\n",
        "generator = nn.Sequential(\n",
        "    nn.Linear(codings_dim, 128), nn.ReLU(),\n",
        "    nn.Linear(128, 256), nn.ReLU(),\n",
        "    nn.Linear(256, 1 * 28 * 28), nn.Sigmoid(),\n",
        "    nn.Unflatten(dim=1, unflattened_size=(1, 28, 28))).to(device)\n",
        "discriminator = nn.Sequential(\n",
        "    nn.Flatten(),\n",
        "    nn.Linear(1 * 28 * 28, 256), nn.ReLU(),\n",
        "    nn.Linear(256, 128), nn.ReLU(),\n",
        "    nn.Linear(128, 1), nn.Sigmoid()).to(device)"
      ],
      "metadata": {
        "id": "UttZ_M1T6bm-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_gan(generator, discriminator, train_loader, codings_dim, n_epochs=20,\n",
        "              g_lr=1e-3, d_lr=5e-4):\n",
        "    criterion = nn.BCELoss()\n",
        "    generator_opt = torch.optim.NAdam(generator.parameters(), lr=g_lr)\n",
        "    discriminator_opt = torch.optim.NAdam(discriminator.parameters(), lr=d_lr)\n",
        "    for epoch in range(n_epochs):\n",
        "        print(f\"Epoch {epoch + 1}/{n_epochs}\", end=\"\")\n",
        "        for real_images, _ in train_loader:\n",
        "            real_images = real_images.to(device)\n",
        "            pred_real = discriminator(real_images)\n",
        "            batch_size = real_images.size(0)\n",
        "            ones = torch.ones(batch_size, 1, device=device)\n",
        "            real_loss = criterion(pred_real, ones)\n",
        "            codings = torch.randn(batch_size, codings_dim, device=device)\n",
        "            fake_images = generator(codings).detach()\n",
        "            pred_fake = discriminator(fake_images)\n",
        "            zeros = torch.zeros(batch_size, 1, device=device)\n",
        "            fake_loss = criterion(pred_fake, zeros)\n",
        "            discriminator_loss = real_loss + fake_loss\n",
        "            discriminator_opt.zero_grad()\n",
        "            discriminator_loss.backward()\n",
        "            discriminator_opt.step()\n",
        "\n",
        "            codings = torch.randn(batch_size, codings_dim, device=device)\n",
        "            fake_images = generator(codings)\n",
        "            for p in discriminator.parameters():\n",
        "                p.requires_grad = False\n",
        "            pred_fake = discriminator(fake_images)\n",
        "            generator_loss = criterion(pred_fake, ones)\n",
        "            generator_opt.zero_grad()\n",
        "            generator_loss.backward()\n",
        "            generator_opt.step()\n",
        "            for p in discriminator.parameters():\n",
        "                p.requires_grad = True\n",
        "        print(f\" | discriminator loss: {discriminator_loss.item():.4f}\", end=\"\")\n",
        "        print(f\" | generator loss: {generator_loss.item():.4f}\")\n",
        "        if epoch % 10 == 0 or epoch == n_epochs - 1:\n",
        "            plot_multiple_images(fake_images.detach(), 8)\n",
        "            plt.show()"
      ],
      "metadata": {
        "id": "3Ukg0ZZ76bpy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_gan(generator, discriminator, train_loader, codings_dim, n_epochs=20,\n",
        "              g_lr=1e-3, d_lr=5e-4):\n",
        "    criterion = nn.BCELoss()\n",
        "    generator_opt = torch.optim.NAdam(generator.parameters(), lr=g_lr)\n",
        "    discriminator_opt = torch.optim.NAdam(discriminator.parameters(), lr=d_lr)\n",
        "    for epoch in range(n_epochs):\n",
        "        print(f\"Epoch {epoch + 1}/{n_epochs}\", end=\"\")\n",
        "        for real_images, _ in train_loader:\n",
        "            real_images = real_images.to(device)\n",
        "            pred_real = discriminator(real_images)\n",
        "            batch_size = real_images.size(0)\n",
        "            ones = torch.ones(batch_size, 1, device=device)\n",
        "            real_loss = criterion(pred_real, ones)\n",
        "            codings = torch.randn(batch_size, codings_dim, device=device)\n",
        "            fake_images = generator(codings).detach()\n",
        "            pred_fake = discriminator(fake_images)\n",
        "            zeros = torch.zeros(batch_size, 1, device=device)\n",
        "            fake_loss = criterion(pred_fake, zeros)\n",
        "            discriminator_loss = real_loss + fake_loss\n",
        "            discriminator_opt.zero_grad()\n",
        "            discriminator_loss.backward()\n",
        "            discriminator_opt.step()\n",
        "\n",
        "            codings = torch.randn(batch_size, codings_dim, device=device)\n",
        "            fake_images = generator(codings)\n",
        "            for p in discriminator.parameters():\n",
        "                p.requires_grad = False\n",
        "            pred_fake = discriminator(fake_images)\n",
        "            generator_loss = criterion(pred_fake, ones)\n",
        "            generator_opt.zero_grad()\n",
        "            generator_loss.backward()\n",
        "            generator_opt.step()\n",
        "            for p in discriminator.parameters():\n",
        "                p.requires_grad = True\n",
        "        print(f\" | discriminator loss: {discriminator_loss.item():.4f}\", end=\"\")\n",
        "        print(f\" | generator loss: {generator_loss.item():.4f}\")\n",
        "        if epoch % 10 == 0 or epoch == n_epochs - 1:\n",
        "            plot_multiple_images(fake_images.detach(), 8)\n",
        "            plt.show()"
      ],
      "metadata": {
        "id": "EwbgccvP6bsf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(41)  # trying another seed often fixes the issue \n",
        "train_gan(generator, discriminator, train_loader, codings_dim)"
      ],
      "metadata": {
        "id": "GxgkNVs86bvI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)  # extra code  ensures reproducibility\n",
        "n_images = 3 * 7\n",
        "generator.eval()\n",
        "codings = torch.randn(n_images, codings_dim, device=device)\n",
        "with torch.no_grad():\n",
        "    generated_images = generator(codings)"
      ],
      "metadata": {
        "id": "_OwA-UOl6bxu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# extra code  this cell generates Figure 1815\n",
        "plot_multiple_images(generated_images, 7)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "YhRlTxSs6b0T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Deep Convolutional GAN"
      ],
      "metadata": {
        "id": "j6ViM9UZ6n70"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(1)  # extra code  ensures reproducibility\n",
        "\n",
        "dc_codings_dim = 100\n",
        "\n",
        "dc_generator = nn.Sequential(\n",
        "    nn.Linear(dc_codings_dim, 128 * 7 * 7),\n",
        "    nn.Unflatten(dim=1, unflattened_size=(128, 7, 7)),\n",
        "    nn.BatchNorm2d(128),\n",
        "    nn.ConvTranspose2d(128, 64, kernel_size=3, stride=2, padding=1,\n",
        "                       output_padding=1), nn.ReLU(),\n",
        "    nn.BatchNorm2d(64),\n",
        "    nn.ConvTranspose2d(64, 1, kernel_size=3, stride=2, padding=1,\n",
        "                   output_padding=1), nn.Sigmoid()).to(device)\n",
        "dc_discriminator = nn.Sequential(\n",
        "    nn.Conv2d(1, 32, kernel_size=5, stride=2, padding=2), nn.ReLU(),  # 32 x 14 x 14\n",
        "    nn.Dropout(0.4),\n",
        "    nn.Conv2d(32, 64, kernel_size=5, stride=2, padding=2), nn.ReLU(),  # 64 x 7 x 7\n",
        "    nn.Dropout(0.4),\n",
        "    nn.Flatten(),\n",
        "    nn.Linear(64 * 7 * 7, 1), nn.Sigmoid()).to(device)"
      ],
      "metadata": {
        "id": "Uh-6WuoH6b2M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)\n",
        "train_gan(dc_generator, dc_discriminator, train_loader, dc_codings_dim)"
      ],
      "metadata": {
        "id": "oClujQya6qKE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# extra code  this cell generates Figure 1816\n",
        "torch.manual_seed(42)\n",
        "with torch.no_grad():\n",
        "    codings = torch.randn(8, dc_codings_dim, device=device)\n",
        "    generated_images = dc_generator(codings)\n",
        "\n",
        "plot_multiple_images(generated_images, 8)"
      ],
      "metadata": {
        "id": "1FKdBBC36qMw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Diffusion Models"
      ],
      "metadata": {
        "id": "9iekVOTL6tGc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def variance_schedule(T, s=0.008, max_beta=0.999):\n",
        "    t = torch.linspace(0, T, T + 1)\n",
        "    f = torch.cos((t / T + s) / (1 + s) * torch.pi / 2) ** 2\n",
        "    alpha_bars = f / f[0]\n",
        "    betas = (1 - (f[1:] / f[:-1])).clamp(max=max_beta)\n",
        "    betas = torch.cat([torch.zeros(1), betas])  # for easier indexing\n",
        "    alphas = 1 - betas\n",
        "    return alphas, betas, alpha_bars\n",
        "\n",
        "torch.manual_seed(42)  # extra code  for reproducibility\n",
        "T = 4000\n",
        "alphas, betas, alpha_bars = variance_schedule(T)"
      ],
      "metadata": {
        "id": "xrAcVOp56qP9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# extra code  this cell generates Figure 1821\n",
        "plt.figure(figsize=(6, 3))\n",
        "plt.plot(betas, \"r--\", label=r\"$\\beta_t$\")\n",
        "plt.plot(alpha_bars, \"b\", label=r\"$\\bar{\\alpha}_t$\")\n",
        "plt.axis([0, T, 0, 1.01])\n",
        "plt.grid(True)\n",
        "plt.xlabel(\"$t$\")\n",
        "plt.ylabel(r\"Value\")\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "zYJCEn8q6qTL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def forward_diffusion(x0, t):\n",
        "    eps = torch.randn_like(x0)  # this unscaled noise will be the target\n",
        "    xt = alpha_bars[t].sqrt() * x0 + (1 - alpha_bars[t]).sqrt() * eps\n",
        "    return xt, eps"
      ],
      "metadata": {
        "id": "gjOyqHKG6qWX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DiffusionSample(namedtuple(\"DiffusionSampleBase\", [\"xt\", \"t\"])):\n",
        "    def to(self, device):\n",
        "        return DiffusionSample(self.xt.to(device), self.t.to(device))"
      ],
      "metadata": {
        "id": "4He1vAeb6qZM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DiffusionDataset:\n",
        "    def __init__(self, dataset):\n",
        "        self.dataset = dataset\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        x0, _ = self.dataset[i]\n",
        "        x0 = (x0 * 2) - 1  # scale from 1 to +1\n",
        "        t = torch.randint(1, T + 1, size=[1])\n",
        "        xt, eps = forward_diffusion(x0, t)\n",
        "        return DiffusionSample(xt, t), eps\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n",
        "\n",
        "train_set = DiffusionDataset(train_data)  # wrap Fashion MNIST\n",
        "train_loader = DataLoader(train_set, batch_size=32, shuffle=True)"
      ],
      "metadata": {
        "id": "w-7gSbrP6qb5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "valid_set = DiffusionDataset(valid_data)\n",
        "valid_loader = DataLoader(valid_set, batch_size=32)"
      ],
      "metadata": {
        "id": "uO7ESTLY6qek"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# extra code  just a quick sanity check\n",
        "\n",
        "def original_image(sample, noise):\n",
        "    alpha_bars_t = torch.gather(alpha_bars, dim=0, index=sample.t.squeeze(1))\n",
        "    alpha_bars_t = alpha_bars_t.view(-1, 1, 1, 1)\n",
        "    x0 = (sample.xt - (1 - alpha_bars_t).sqrt() * noise) / alpha_bars_t.sqrt()\n",
        "    return torch.clamp((x0 + 1) / 2, 0, 1)\n",
        "\n",
        "torch.manual_seed(42)\n",
        "sample, eps = next(iter(train_loader))  # get the first batch\n",
        "x0 = original_image(sample, eps).to(device)\n",
        "\n",
        "print(\"Original images\")\n",
        "plot_multiple_images(x0[:8])\n",
        "plt.show()\n",
        "print(\"Time steps:\", sample.t[:8].view(-1).tolist())\n",
        "print(\"Noisy images\")\n",
        "plot_multiple_images(sample.xt[:8])\n",
        "plt.show()\n",
        "print(\"Noise to predict\")\n",
        "plot_multiple_images(eps[:8])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "IE5G-kva6149"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# extra code  implements a custom time encoding layer\n",
        "\n",
        "embed_dim = 64\n",
        "\n",
        "class TimeEncoding(nn.Module):\n",
        "    def __init__(self, T, embed_dim):\n",
        "        super().__init__()\n",
        "        assert embed_dim % 2 == 0, \"embed_dim must be even\"\n",
        "        p = torch.arange(T + 1).unsqueeze(1)\n",
        "        angle = p / 10_000 ** (torch.arange(0, embed_dim, 2) / embed_dim)\n",
        "        te = torch.empty(T + 1, embed_dim)\n",
        "        te[:, 0::2] = torch.sin(angle)\n",
        "        te[:, 1::2] = torch.cos(angle)\n",
        "        self.register_buffer(\"time_encodings\", te)\n",
        "\n",
        "    def forward(self, t):\n",
        "        return self.time_encodings[t]"
      ],
      "metadata": {
        "id": "q-61maKy617n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SeparableConv2d(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size, padding):\n",
        "        super().__init__()\n",
        "        self.depthwise = nn.Conv2d(in_channels, in_channels, kernel_size,\n",
        "                                   padding=padding, groups=in_channels)\n",
        "        self.pointwise = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n",
        "\n",
        "    def forward(self, X):\n",
        "        return self.pointwise(self.depthwise(X))\n",
        "\n",
        "class DiffusionModel(nn.Module):\n",
        "    def __init__(self, T=T, embed_dim=64):\n",
        "        super().__init__()\n",
        "        self.time_encoding = TimeEncoding(T, embed_dim)\n",
        "\n",
        "        # Init\n",
        "        dim = 16\n",
        "        self.pad = nn.ConstantPad2d((3, 3, 3, 3), 0)\n",
        "        self.init_conv = nn.Conv2d(1, dim, kernel_size=3)\n",
        "        self.init_bn = nn.BatchNorm2d(dim)\n",
        "        self.time_adapter_init = nn.Linear(embed_dim, dim)\n",
        "\n",
        "        # Down path\n",
        "        self.down_blocks = nn.ModuleList()\n",
        "        self.skip_convs = nn.ModuleList()\n",
        "        self.time_adapters_down = nn.ModuleList()\n",
        "        in_dim = dim\n",
        "        for dim in (32, 64, 128):\n",
        "            block = nn.Sequential(\n",
        "                nn.ReLU(),\n",
        "                SeparableConv2d(in_dim, dim, kernel_size=3, padding=1),\n",
        "                nn.BatchNorm2d(dim),\n",
        "                nn.ReLU(),\n",
        "                SeparableConv2d(dim, dim, kernel_size=3, padding=1),\n",
        "                nn.BatchNorm2d(dim)\n",
        "            )\n",
        "            skip_conv = nn.Conv2d(in_dim, dim, kernel_size=1, stride=2)\n",
        "            self.down_blocks.append(block)\n",
        "            self.skip_convs.append(skip_conv)\n",
        "            self.time_adapters_down.append(nn.Linear(embed_dim, dim))\n",
        "            in_dim = dim\n",
        "\n",
        "        # Up path\n",
        "        self.up_blocks = nn.ModuleList()\n",
        "        self.skip_up_convs = nn.ModuleList()\n",
        "        self.time_adapters_up = nn.ModuleList()\n",
        "        for dim in (64, 32, 16):\n",
        "            block = nn.Sequential(\n",
        "                nn.ReLU(),\n",
        "                nn.ConvTranspose2d(in_dim, dim, 3, padding=1),\n",
        "                nn.BatchNorm2d(dim),\n",
        "                nn.ReLU(),\n",
        "                nn.ConvTranspose2d(dim, dim, 3, padding=1),\n",
        "                nn.BatchNorm2d(dim)\n",
        "            )\n",
        "            skip_conv = nn.Conv2d(in_dim, dim, kernel_size=1)\n",
        "            self.up_blocks.append(block)\n",
        "            self.skip_up_convs.append(skip_conv)\n",
        "            self.time_adapters_up.append(nn.Linear(embed_dim, dim))\n",
        "            in_dim = dim * 3  # because of concatenation with cross skip\n",
        "\n",
        "        self.final_conv = nn.Conv2d(in_dim, 1, 3, padding=1)\n",
        "\n",
        "    def forward(self, sample):\n",
        "        if not isinstance(sample, DiffusionSample):\n",
        "            print(repr(sample))\n",
        "        time_enc = self.time_encoding(sample.t.squeeze(1))  # [batch, embed_dim]\n",
        "        z = self.pad(sample.xt)\n",
        "        z = F.relu(self.init_bn(self.init_conv(z)))\n",
        "        z = z + self.time_adapter_init(time_enc)[:, :, None, None]\n",
        "        skip = z\n",
        "        cross_skips = []\n",
        "\n",
        "        # Downsampling path\n",
        "        for block, skip_conv, time_adapter in zip(\n",
        "                self.down_blocks, self.skip_convs, self.time_adapters_down):\n",
        "            z = block(z)\n",
        "            cross_skips.append(z)\n",
        "            z = F.max_pool2d(z, 3, stride=2, padding=1)\n",
        "            skip_link = skip_conv(skip)\n",
        "            z = z + skip_link\n",
        "            z = z + time_adapter(time_enc)[:, :, None, None]\n",
        "            skip = z\n",
        "\n",
        "        # Upsampling path\n",
        "        for block, skip_up_conv, time_adapter in zip(\n",
        "                self.up_blocks, self.skip_up_convs, self.time_adapters_up):\n",
        "            z = block(z)\n",
        "            z = F.interpolate(z, scale_factor=2, mode=\"nearest\")\n",
        "            skip_link = F.interpolate(skip, scale_factor=2, mode=\"nearest\")\n",
        "            skip_link = skip_up_conv(skip_link)\n",
        "            z = z + skip_link\n",
        "            z = z + time_adapter(time_enc)[:, :, None, None]\n",
        "            cross_skip = cross_skips.pop()\n",
        "            z = torch.cat([z, cross_skip], dim=1)\n",
        "            skip = z\n",
        "\n",
        "        out = self.final_conv(z)\n",
        "        out = out[:, :, 2:-2, 2:-2]  # cropping\n",
        "        return out.contiguous()"
      ],
      "metadata": {
        "id": "RRejdt7261-Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)\n",
        "diffusion_model = DiffusionModel().to(device)\n",
        "huber = nn.HuberLoss()\n",
        "optimizer = torch.optim.NAdam(diffusion_model.parameters(), lr=3e-3)\n",
        "rmse = torchmetrics.MeanSquaredError(squared=False).to(device)\n",
        "history = train(diffusion_model, optimizer, huber, rmse, train_loader,\n",
        "                valid_loader, n_epochs=20)"
      ],
      "metadata": {
        "id": "pvk-NdsU62Bz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_ddpm(model, batch_size=32):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        xt = torch.randn([batch_size, 1, 28, 28], device=device)\n",
        "        for t in range(T, 0, -1):\n",
        "            print(f\"\\rt = {t}\", end=\" \")  # extra code  show progress\n",
        "            alpha_t = alphas[t]\n",
        "            beta_t = betas[t]\n",
        "            alpha_bar_t = alpha_bars[t]\n",
        "            noise = (torch.randn(xt.shape, device=device)\n",
        "                     if t > 1 else torch.zeros(xt.shape, device=device))\n",
        "            t_batch = torch.full((batch_size, 1), t, device=device)\n",
        "            sample = DiffusionSample(xt, t_batch)\n",
        "            eps_pred = model(sample)\n",
        "            xt = (1 / alpha_t.sqrt()\n",
        "                  * (xt - beta_t / (1 - alpha_bar_t).sqrt() * eps_pred)\n",
        "                  + (1 - alpha_t).sqrt() * noise)\n",
        "        return torch.clamp((xt + 1) / 2, 0, 1)\n",
        "\n",
        "torch.manual_seed(42)  # extra code  ensures reproducibility\n",
        "X_gen = generate_ddpm(diffusion_model)  # generated images"
      ],
      "metadata": {
        "id": "X28Efn0y62Fh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_multiple_images(X_gen, 8)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "oYF_GKk562Il"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_ddim(model, batch_size=32, num_steps=50, eta=0.85):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        xt = torch.randn([batch_size, 1, 28, 28], device=device)\n",
        "        times = torch.linspace(T - 1, 0, steps=num_steps + 1).long().tolist()\n",
        "        for t, t_prev in zip(times[:-1], times[1:]):\n",
        "            print(f\"\\rt = {t}\", end=\" \")\n",
        "            t_batch = torch.full((batch_size, 1), t, device=device)\n",
        "            sample = DiffusionSample(xt, t_batch)\n",
        "            eps_pred = model(sample)\n",
        "            x0 = ((xt - (1 - alpha_bars[t]).sqrt() * eps_pred)\n",
        "                  / (alpha_bars[t].sqrt()))\n",
        "            abar_t_prev = alpha_bars[t_prev]\n",
        "            variance = eta * (1 - abar_t_prev) / (1 - alpha_bars[t]) * betas[t]\n",
        "            sigma_t = variance.sqrt()\n",
        "            pred_dir = (1 - abar_t_prev - sigma_t**2).sqrt() * eps_pred\n",
        "            noise = torch.randn_like(xt)\n",
        "            xt = abar_t_prev.sqrt() * x0 + pred_dir + sigma_t * noise\n",
        "\n",
        "        return torch.clamp((xt + 1) / 2, 0, 1)  # from [1, 1] range to [0, 1]\n",
        "\n",
        "torch.manual_seed(42)  # extra code  ensures reproducibility\n",
        "X_gen_ddim = generate_ddim(diffusion_model, num_steps=500)"
      ],
      "metadata": {
        "id": "EdL0VkWm62LY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_multiple_images(X_gen_ddim, 8)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "U7zOisfi62Na"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from diffusers import AutoPipelineForText2Image\n",
        "\n",
        "pipe = AutoPipelineForText2Image.from_pretrained(\n",
        "    \"stabilityai/sd-turbo\", variant=\"fp16\", dtype=torch.float16)\n",
        "pipe.to(device)\n",
        "prompt = \"A closeup photo of an orangutan reading a book\""
      ],
      "metadata": {
        "id": "1kLX0lkA7BW8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)\n",
        "image = pipe(prompt=prompt, num_inference_steps=1, guidance_scale=0.0).images[0]\n",
        "image.save(\"my_orangutan_reading.png\")\n",
        "plt.imshow(image)\n",
        "plt.axis(\"off\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ux82A8WG7BZ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7DgKjaGy7Bck"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "USXM3OMp7BfP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1TaXLRfH7Bh6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "y2TiWXIG7BkV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "36vO0QTz7Bm8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}